{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xmltodict\n",
    "from bs4 import BeautifulSoup\n",
    "from xml.etree import ElementTree as ET\n",
    "import time \n",
    "\n",
    "\n",
    "def search(search_string):\n",
    "    result = []\n",
    "    start = 0 \n",
    "    #This function passes your queries, separated by commas, in addition to the record you'd like to start with. \n",
    "    RECHERCHE_BASEURL = 'https://gallica.bnf.fr/SRU?operation=searchRetrieve&version=1.2&startRecord=' + str(start)\n",
    "    RECHERCHE_BASEURL1 = '&maximumRecords=100&page=' + str(start)\n",
    "    RECHERCHE_BASEURL2 = '&query=%28gallica%20adj%20'\n",
    "    \n",
    "    search_string = search_string.replace('\"','%22')\n",
    "    search_string = search_string.replace(' ', '%20') # \"%22gateaux%20de%20pommes%22\"\n",
    "\n",
    "    url = \"\".join([RECHERCHE_BASEURL, RECHERCHE_BASEURL1, RECHERCHE_BASEURL2, search_string]) #, START_REC])\n",
    "   # print(url)\n",
    "    s = requests.get(url, stream=True)\n",
    "    soup = BeautifulSoup(s.content,\"lxml-xml\")\n",
    "    tree = ET.ElementTree(ET.fromstring(s.text))\n",
    "    root = tree.getroot()\n",
    "    number_of_results=root.find('.//{http://www.loc.gov/zing/srw/}numberOfRecords')\n",
    "   # print(number_of_results.text)\n",
    "    number_of_pages = int(number_of_results.text) / 100\n",
    "    \n",
    "    for page in range(0,int(number_of_pages)): \n",
    "        page_no = str(page) \n",
    "    #    print(page_no)\n",
    "        RECHERCHE_BASEURL = 'https://gallica.bnf.fr/SRU?operation=searchRetrieve&version=1.2&startRecord=' + str(start) \n",
    "        RECHERCHE_BASEURL1 = '&maximumRecords=100&page=' + str(page)\n",
    "        url = \"\".join([RECHERCHE_BASEURL, RECHERCHE_BASEURL1, RECHERCHE_BASEURL2, search_string]) #, START_REC])\n",
    "    #    print(url)\n",
    "        try:\n",
    "            s = requests.get(url, stream=True)\n",
    "            soup = BeautifulSoup(s.content,\"lxml-xml\")\n",
    "            tree = ET.ElementTree(ET.fromstring(s.text))\n",
    "            root = tree.getroot()\n",
    "            article_id = \"\"\n",
    "            articletype = \"\"\n",
    "            title = \"\"\n",
    "            publicationdate = \"\" \n",
    "            publicationname = \"\"\n",
    "            page = \"\" \n",
    "            ocrquality = \"\"\n",
    "            for child in root.findall('.//{http://www.openarchives.org/OAI/2.0/oai_dc/}dc'):\n",
    "                for item in child:\n",
    "                    if item.tag == '{http://purl.org/dc/elements/1.1/}title':\n",
    "                        title = item.text\n",
    "                    if item.tag == '{http://purl.org/dc/elements/1.1/}identifier':\n",
    "                        if 'ark' in item.text:\n",
    "                            article_id = item.text\n",
    "                    if item.tag == '{http://purl.org/dc/elements/1.1/}date':\n",
    "                        publicationdate = item.text\n",
    "                    if item.tag == '{http://purl.org/dc/elements/1.1/}type':\n",
    "                        articletype = item.text\n",
    "                    if item.tag == '{http://www.kb.nl/ddd}papertitle':\n",
    "                        publicationname = item.text\n",
    "                    if item.tag == '{http://www.kb.nl/ddd}page':\n",
    "                        page = item.text\n",
    "                    if item.tag == 'ocrquality':\n",
    "                        ocrquality = item.text \n",
    "\n",
    "                resultline = search_string + \"\\t\" + article_id + \"\\t\" + publicationdate + \"\\t\" + title + \"\\t\" + articletype + \"\\t\" + publicationname + \"\\t\" + page + \"\\n\"\n",
    "                result.append(resultline)\n",
    "                s = {}\n",
    "            sleep(1)\n",
    "        except:\n",
    "     #       print(s)\n",
    "            pass\n",
    "        start = start + 100\n",
    "   # print(\"number_of_results \", len(result))\n",
    "    return(result)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texte_brut(id):\n",
    "    TEXTEBRUT_BASEURL = 'https://gallica.bnf.fr/ark:/'\n",
    "    \n",
    "    url = \"\".join([TEXTEBRUT_BASEURL, id, '.texteBrut'])\n",
    "    #print(url)\n",
    "    \n",
    "    s = requests.get(url, stream=True)\n",
    "    soup = BeautifulSoup(s.content,\"lxml-xml\")\n",
    "    #print(soup.prettify())\n",
    "    text= str(soup.hr)\n",
    "    return(text)\n",
    "\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"tarte aux pommes\",\"tarte de pommes\", \"tarte tatin\", \"gateau de pommes\", \"gateau aux pommes\"]\n",
    "\n",
    "os.chdir('/Users/marieke/Work/CommoditiesHandbook/french_documents')\n",
    "print(os.getcwd())\n",
    "\n",
    "for query in queries: \n",
    "    result = search(query)\n",
    "    print(query, len(result))\n",
    "   # outputfilename = '/Users/marieke/Work/CommoditiesHandbook/Query_results_' + query.replace(\" \", \"_\") + '.tsv' \n",
    "   # f = open(outputfilename, 'w')\n",
    "    for r in result:\n",
    "   #     f.write(r)\n",
    "        elems = r.split(\"\\t\")\n",
    "        print(elems[1])\n",
    "        parts = elems[1].split('/')\n",
    "        if parts[5].startswith(\"bpt6\") or parts[5].startswith(\"btv1\"):\n",
    "            id = parts[4] + \"/\" + parts[5]\n",
    "   #     print(id)\n",
    "            if not os.path.exists(parts[4]):\n",
    "                os.makedirs(parts[4])\n",
    "            os.chdir(parts[4])\n",
    "            filename = elems[0].replace('%20', \"_\") + \"_\" + parts[5] + \".txt\" \n",
    "            f = open(filename, 'w')\n",
    "   #     print(filename)\n",
    "            fulltext = texte_brut(id)\n",
    "            clean_text = remove_html_tags(fulltext)\n",
    "            f.write(elems[1] + \"\\n\" + elems[2] + \"\\n\")\n",
    "            f.write(clean_text)\n",
    "            f.close()\n",
    "            os.chdir(\"/Users/marieke/Work/CommoditiesHandbook/french_documents\")\n",
    "       # print(os.getcwd())\n",
    "            time.sleep(1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
